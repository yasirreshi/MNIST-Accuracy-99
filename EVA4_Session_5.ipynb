{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MNIST Digit Classification using a Convolutional Neural Network\n",
        "\n",
        "### 1. Introduction and Goal Definition\n",
        "\n",
        "*   **The Problem:** This notebook aims to build and train a Convolutional Neural Network (CNN) to accurately classify handwritten digits from the well-known MNIST dataset.\n",
        "*   **The Dataset:** The MNIST dataset consists of 60,000 training images and 10,000 testing images of handwritten digits (0-9), each of size 28x28 pixels.\n",
        "*   **The Goal:** The primary objective is to design a model that achieves high classification accuracy (aiming for over 99.4%) on the test set while keeping the model lightweight (under 20,000 parameters). This is a common challenge in deploying models to resource-constrained environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Libraries\n",
        "This cell imports the necessary libraries for building and training the neural network. We import `torch` for tensor operations, `torch.nn` for building neural network layers, `torch.optim` for optimization algorithms, and `torchvision` for datasets and image transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Configuration and Hyperparameters\n",
        "\n",
        "This cell groups all the key hyperparameters and settings in one place. This is a crucial practice for readability and makes it easy to experiment with different values without searching through the entire notebook.\n",
        "\n",
        "*   **`EPOCHS`**: The number of times the model will see the entire training dataset.\n",
        "*   **`BATCH_SIZE`**: The number of training samples to process before the model's parameters are updated.\n",
        "*   **`LEARNING_RATE`**: Controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
        "*   **`MOMENTUM`**: Helps accelerate SGD in the relevant direction and dampens oscillations.\n",
        "*   **`USE_CUDA` and `DEVICE`**: Checks if a GPU is available and sets the device for training accordingly to speed up computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: xpu\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "has_xpu_attr = getattr(torch, \"xpu\", None) is not None\n",
        "USE_XPU = bool(has_xpu_attr and torch.xpu.is_available())\n",
        "if USE_XPU:\n",
        "    DEVICE = torch.device(\"xpu\")\n",
        "elif USE_CUDA:\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "print(\"Using device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Input Block\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05)\n",
        "        ) # output_size = 26\n",
        "\n",
        "        # CONVOLUTION BLOCK 1\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05)\n",
        "        ) # output_size = 24\n",
        "\n",
        "        # TRANSITION BLOCK 1\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 12\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(3, 3), padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU()\n",
        "        ) # output_size = 12\n",
        "\n",
        "        # CONVOLUTION BLOCK 2\n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05)\n",
        "        ) # output_size = 10\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=0, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05)\n",
        "        ) # output_size = 8\n",
        "\n",
        "        # OUTPUT BLOCK\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        ) # output_size = 1\n",
        "\n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding=0, bias=False)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Define the Neural Network Architecture\n",
        "\n",
        "This cell defines the neural network architecture. The model is designed to be lightweight and efficient, using several key concepts:\n",
        "\n",
        "*   **Why Convolutional Layers?** CNNs are the industry standard for computer vision tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from images, from simple edges in the first layers to complex patterns in deeper layers.\n",
        "*   **Why Batch Normalization?** Batch Normalization is a crucial technique for training deep networks. It normalizes the output of a previous activation layer by subtracting the batch mean and dividing by the batch standard deviation. This stabilizes the learning process, accelerates training, and can also act as a regularizer, reducing the need for other regularization techniques like dropout.\n",
        "*   **Why Dropout?** Dropout is a powerful regularization technique to prevent overfitting. During training, it randomly sets a fraction of neuron activations to zero at each update step. This forces the network to learn more robust features and prevents neurons from co-adapting too much.\n",
        "*   **Why Global Average Pooling (GAP)?** Instead of a traditional fully connected layer, we use GAP at the end of the network. GAP takes the average of each feature map and feeds the resulting vector directly into the softmax layer. This has two major benefits:\n",
        "    1.  It drastically reduces the number of parameters in the model, which helps to prevent overfitting and makes the model much lighter.\n",
        "    2.  It is more native to the convolution structure and enforces correspondences between feature maps and categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xdydjYTZFyi3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in c:\\users\\yasir\\anaconda3\\envs\\torch-xpu\\lib\\site-packages (1.5.1)\n",
            "Torchsummary does not support XPU, skipping model summary.\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "model = Net().to(DEVICE)\n",
        "if DEVICE.type == \"xpu\":\n",
        "    print(\"Torchsummary does not support XPU, skipping model summary.\")\n",
        "else:\n",
        "    summary(model, input_size=(1, 28, 28), device=DEVICE.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Summary\n",
        "This cell prints a summary of the model architecture, showing the layers, output shapes, and the number of parameters. This is useful for verifying the model structure and checking the parameter count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'use_cuda' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m torch.manual_seed(\u001b[32m1\u001b[39m)\n\u001b[32m      2\u001b[39m batch_size = \u001b[32m128\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m kwargs = {\u001b[33m'\u001b[39m\u001b[33mnum_workers\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpin_memory\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m \u001b[43muse_cuda\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m      5\u001b[39m train_loader = torch.utils.data.DataLoader(\n\u001b[32m      6\u001b[39m     datasets.MNIST(\u001b[33m'\u001b[39m\u001b[33m../data\u001b[39m\u001b[33m'\u001b[39m, train=\u001b[38;5;28;01mTrue\u001b[39;00m, download=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      7\u001b[39m                     transform=transforms.Compose([\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m                     ])),\n\u001b[32m     11\u001b[39m     batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs)\n\u001b[32m     12\u001b[39m test_loader = torch.utils.data.DataLoader(\n\u001b[32m     13\u001b[39m     datasets.MNIST(\u001b[33m'\u001b[39m\u001b[33m../data\u001b[39m\u001b[33m'\u001b[39m, train=\u001b[38;5;28;01mFalse\u001b[39;00m, transform=transforms.Compose([\n\u001b[32m     14\u001b[39m                         transforms.ToTensor(),\n\u001b[32m     15\u001b[39m                         transforms.Normalize((\u001b[32m0.1307\u001b[39m,), (\u001b[32m0.3081\u001b[39m,))\n\u001b[32m     16\u001b[39m                     ])),\n\u001b[32m     17\u001b[39m     batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs)\n",
            "\u001b[31mNameError\u001b[39m: name 'use_cuda' is not defined"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if USE_CUDA else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading and Train/Test Split\n",
        "This cell is responsible for loading the MNIST dataset and splitting it into training and testing sets.\n",
        "\n",
        "- **`datasets.MNIST`**: This function from `torchvision` downloads and loads the MNIST dataset. We create two instances: one for training (`train=True`) and one for testing (`train=False`).\n",
        "- **`transforms.Compose`**: This is used to apply a sequence of transformations to the data.\n",
        "  - **`transforms.ToTensor()`**: Converts the images from PIL Image format to PyTorch Tensors.\n",
        "  - **`transforms.Normalize()`**: Normalizes the tensor image with a given mean and standard deviation. Normalization helps the model to converge faster. The values `(0.1307,)` and `(0.3081,)` are the pre-computed mean and standard deviation for the MNIST dataset.\n",
        "- **`torch.utils.data.DataLoader`**: This wraps the dataset and provides an iterable over it. It handles batching, shuffling, and loading data in parallel.\n",
        "  - **`batch_size`**: The number of samples per batch.\n",
        "  - **`shuffle=True`**: Randomly shuffles the data at every epoch. This is important for training to ensure that the model does not learn the order of the training data.\n",
        "  - **`num_workers`**: The number of subprocesses to use for data loading.\n",
        "  - **`pin_memory`**: If `True`, the data loader will copy tensors into CUDA pinned memory before returning them, which can speed up data transfer to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
        "    ax.set_title(f\"Label: {labels[i].item()}\")\n",
        "    ax.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Visualizing the Data\n",
        "\n",
        "Before we train the model, it's a good practice to visualize a few samples from the dataset. This serves as a sanity check to ensure our data is loaded and processed correctly and gives us an intuition for the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Loading and Preprocessing\n",
        "This cell sets up the data loaders for the MNIST dataset. It includes transformations to convert the images to tensors and normalize them. The data is split into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "train_losses = []\n",
        "train_acc = []\n",
        "test_losses = []\n",
        "test_acc = []\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  processed = 0\n",
        "  for batch_idx, (data, target) in enumerate(pbar):\n",
        "    # get samples\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes.\n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "\n",
        "    pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    processed += len(data)\n",
        "\n",
        "    pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "  train_acc.append(100*correct/processed)\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "    test_acc.append(100. * correct / len(test_loader.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and Testing Functions\n",
        "This cell defines the `train` and `test` functions.\n",
        "- **`train` function:** This function iterates over the training data, performs forward and backward passes, and updates the model parameters.\n",
        "- **`test` function:** This function evaluates the model on the test data and prints the test loss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMWbLWO6FuHb"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = Net().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "EPOCHS = 20\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
        "    test(model, DEVICE, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training\n",
        "This cell initializes the model and the optimizer, and then trains the model for a specified number of epochs. The `train` and `test` functions are called in a loop to train the model and evaluate its performance after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "outputs": [],
      "source": [
        "!pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
        "axs[0, 0].plot(train_losses)\n",
        "axs[0, 0].set_title(\"Training Loss\")\n",
        "axs[1, 0].plot(train_acc)\n",
        "axs[1, 0].set_title(\"Training Accuracy\")\n",
        "axs[0, 1].plot(test_losses)\n",
        "axs[0, 1].set_title(\"Test Loss\")\n",
        "axs[1, 1].plot(test_acc)\n",
        "axs[1, 1].set_title(\"Test Accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting Results\n",
        "This cell plots the training and testing loss and accuracy over the epochs. This is a standard way to visualize the performance of a machine learning model.\n",
        "- **Training Loss:** Should decrease over time, indicating that the model is learning from the training data.\n",
        "- **Training Accuracy:** Should increase over time, indicating that the model is getting better at classifying the training data.\n",
        "- **Test Loss:** Should also decrease over time, but if it starts to increase while the training loss is still decreasing, it's a sign of overfitting.\n",
        "- **Test Accuracy:** This is the primary metric for evaluating the model's performance on unseen data. It should increase and then plateau."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find and plot misclassified images\n",
        "model.eval()\n",
        "misclassified_images = []\n",
        "misclassified_labels = []\n",
        "correct_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        misclassified_mask = (pred.eq(target.view_as(pred)) == False).squeeze()\n",
        "        \n",
        "        misclassified_images.extend(data[misclassified_mask])\n",
        "        misclassified_labels.extend(pred[misclassified_mask])\n",
        "        correct_labels.extend(target.view_as(pred)[misclassified_mask])\n",
        "\n",
        "# Plot some misclassified images\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
        "fig.suptitle(\"Misclassified Images\")\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < len(misclassified_images) and i < 10: # Plot up to 10 misclassified images\n",
        "        ax.imshow(misclassified_images[i].cpu().squeeze(), cmap='gray')\n",
        "        ax.set_title(f\"Pred: {misclassified_labels[i].item()}, True: {correct_labels[i].item()}\")\n",
        "        ax.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Visualizing Misclassifications\n",
        "\n",
        "While overall accuracy is a good measure, it's often more insightful to analyze where the model is making mistakes. This is a critical step in industry for model improvement. By looking at the images the model gets wrong, we can gain insights into its weaknesses. For example, does it consistently confuse certain pairs of digits (like '4' and '9', or '7' and '1')? This analysis can guide further improvements, such as data augmentation or architectural changes."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch-xpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
